{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv('C:\\\\Users\\\\acer\\\\Deep Learning - Chetan Bhaiya\\\\data1.csv', header = None)\n",
    "#print(df)\n",
    "#print(df2)\n",
    "df2 = pd.read_csv('C:\\\\Users\\\\acer\\\\Deep Learning - Chetan Bhaiya\\\\label1.csv', header = None)\n",
    "\n",
    "data1 = np.asarray(df)\n",
    "label1 = np.asarray(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueElements = np.unique(label1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 1. 0.]]\n",
      "\n",
      " [[0. 0. 1. 0. 0.]]\n",
      "\n",
      " [[0. 0. 1. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 1. 0. 0.]]\n",
      "\n",
      " [[0. 0. 1. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "OHElabel = np.eye(len(uniqueElements))[label1]\n",
    "print(OHElabel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 1, 5)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(OHElabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "OHElabel = np.reshape(OHElabel,[20000,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(OHElabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the data set into TRAINING-VALIDATION-TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 5)\n",
      "(2000, 5)\n",
      "(2000, 5)\n",
      "(16000, 20)\n",
      "(2000, 20)\n",
      "(2000, 20)\n"
     ]
    }
   ],
   "source": [
    "eightyP = int((0.8)*len(OHElabel))\n",
    "tenP = int((0.1)*len(OHElabel))\n",
    "\n",
    "trainingL = OHElabel[:eightyP]\n",
    "validationL = OHElabel[eightyP: eightyP + tenP]\n",
    "testingL = OHElabel[eightyP+tenP:]\n",
    "\n",
    "trainingData = data1[:eightyP]\n",
    "validationData = data1[eightyP: eightyP + tenP]\n",
    "testingData = data1[eightyP+tenP:]\n",
    "\n",
    "print(np.shape(trainingL))\n",
    "print(np.shape(validationL))  \n",
    "print(np.shape(testingL))\n",
    "\n",
    "print(np.shape(trainingData))\n",
    "print(np.shape(validationData))  \n",
    "print(np.shape(testingData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "#initialising weights and biases\n",
    "h = int(input())    #no of neurons in inputs hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = np.zeros(h)    #20 biases of hidden layer\n",
    "b2 = np.zeros(5)    #h biases of output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = np.random.normal(0, 1/np.sqrt(20), (20,h))     #for layer 1\n",
    "w2 = np.random.normal(0, 1/np.sqrt(h), (h,5))       #for hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actFunc(haha,choose):\n",
    "    if (choose==1):    #Sigmoid\n",
    "        return (1/(1+np.exp(haha)))\n",
    "    elif (choose==2):    #ReLU\n",
    "        return np.max(0,haha)\n",
    "    elif (choose==3):    #tanh\n",
    "        return np.tanh(haha)\n",
    "    elif (choose==4):    #softmax\n",
    "        temp=np.exp(haha)\n",
    "        return temp/np.sum(temp,axis=1,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n"
     ]
    }
   ],
   "source": [
    "alpha = float(input())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "#cost funtion\n",
    "batch = int(input())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross entropy function\n",
    "def CostFun(y,t):\n",
    "    return ( (1-t)*np.log(1-y) - t*np.log(y) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardFun(data):\n",
    "    a0=data\n",
    "    z1=np.dot(a0,w1)+b1\n",
    "    a1=actFunc(z1,3)    #tanh\n",
    "    z2=np.dot(a1,w2)+b2\n",
    "    a2=actFunc(z2,4)    #we called softmax because we wanted the output of our last layer in probability\n",
    "    return a0,a1,a2,z1,z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy fun\n",
    "def accuracyFun(output,labelData,dataSize):\n",
    "    maxInRow=np.argmax(output, axis=1)\n",
    "    maxInLabels=np.argmax(labelData, axis=1)\n",
    "    haha=np.equal(maxInRow,maxInLabels)\n",
    "    count=np.sum(haha)\n",
    "    \n",
    "    return (count/dataSize)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost nan\n",
      "training accuracy 91.58125\n",
      "cost nan\n",
      "training accuracy 91.59375\n",
      "cost nan\n",
      "training accuracy 91.59375\n",
      "cost nan\n",
      "training accuracy 91.60000000000001\n",
      "cost nan\n",
      "training accuracy 91.60000000000001\n",
      "cost nan\n",
      "training accuracy 91.60625\n",
      "cost nan\n",
      "training accuracy 91.60625\n",
      "cost nan\n",
      "training accuracy 91.60625\n",
      "cost nan\n",
      "training accuracy 91.61875\n",
      "cost nan\n",
      "training accuracy 91.63125\n",
      "cost nan\n",
      "training accuracy 91.6375\n",
      "cost nan\n",
      "training accuracy 91.63125\n",
      "cost nan\n",
      "training accuracy 91.61875\n",
      "cost nan\n",
      "training accuracy 91.6125\n",
      "cost nan\n",
      "training accuracy 91.625\n",
      "cost nan\n",
      "training accuracy 91.625\n",
      "cost nan\n",
      "training accuracy 91.63125\n",
      "cost nan\n",
      "training accuracy 91.61875\n",
      "cost nan\n",
      "training accuracy 91.63125\n",
      "cost nan\n",
      "training accuracy 91.63125\n",
      "cost nan\n",
      "training accuracy 91.63125\n",
      "cost nan\n",
      "training accuracy 91.64375\n",
      "cost nan\n",
      "training accuracy 91.66250000000001\n",
      "cost nan\n",
      "training accuracy 91.66875\n",
      "cost nan\n",
      "training accuracy 91.675\n",
      "cost nan\n",
      "training accuracy 91.66250000000001\n",
      "cost nan\n",
      "training accuracy 91.66875\n",
      "cost nan\n",
      "training accuracy 91.66875\n",
      "cost nan\n",
      "training accuracy 91.66875\n",
      "cost nan\n",
      "training accuracy 91.675\n",
      "cost nan\n",
      "training accuracy 91.675\n",
      "cost nan\n",
      "training accuracy 91.6875\n",
      "cost nan\n",
      "training accuracy 91.7\n",
      "cost nan\n",
      "training accuracy 91.7\n",
      "cost nan\n",
      "training accuracy 91.70625\n",
      "cost nan\n",
      "training accuracy 91.71249999999999\n",
      "cost nan\n",
      "training accuracy 91.70625\n",
      "cost nan\n",
      "training accuracy 91.71249999999999\n",
      "cost nan\n",
      "training accuracy 91.71875\n",
      "cost nan\n",
      "training accuracy 91.73125\n",
      "cost nan\n",
      "training accuracy 91.74375\n",
      "cost nan\n",
      "training accuracy 91.73750000000001\n",
      "cost nan\n",
      "training accuracy 91.73750000000001\n",
      "cost nan\n",
      "training accuracy 91.74375\n",
      "cost nan\n",
      "training accuracy 91.76875\n",
      "cost nan\n",
      "training accuracy 91.77499999999999\n",
      "cost nan\n",
      "training accuracy 91.7625\n",
      "cost nan\n",
      "training accuracy 91.75625\n",
      "cost nan\n",
      "training accuracy 91.75\n",
      "cost nan\n",
      "training accuracy 91.75625\n",
      "cost nan\n",
      "training accuracy 91.75625\n",
      "cost nan\n",
      "training accuracy 91.75625\n",
      "cost nan\n",
      "training accuracy 91.75625\n",
      "cost nan\n",
      "training accuracy 91.74375\n",
      "cost nan\n",
      "training accuracy 91.75625\n",
      "cost nan\n",
      "training accuracy 91.75\n",
      "cost nan\n",
      "training accuracy 91.75625\n",
      "cost nan\n",
      "training accuracy 91.7625\n",
      "cost nan\n",
      "training accuracy 91.75625\n",
      "cost nan\n",
      "training accuracy 91.75\n",
      "cost nan\n",
      "training accuracy 91.75\n",
      "cost nan\n",
      "training accuracy 91.75\n",
      "cost nan\n",
      "training accuracy 91.75\n",
      "cost nan\n",
      "training accuracy 91.75\n",
      "cost nan\n",
      "training accuracy 91.7625\n",
      "cost nan\n",
      "training accuracy 91.76875\n",
      "cost nan\n",
      "training accuracy 91.76875\n",
      "cost nan\n",
      "training accuracy 91.7625\n",
      "cost nan\n",
      "training accuracy 91.75625\n",
      "cost nan\n",
      "training accuracy 91.77499999999999\n",
      "cost nan\n",
      "training accuracy 91.7875\n",
      "cost nan\n",
      "training accuracy 91.78125\n",
      "cost nan\n",
      "training accuracy 91.78125\n",
      "cost nan\n",
      "training accuracy 91.7875\n",
      "cost nan\n",
      "training accuracy 91.79375\n",
      "cost nan\n",
      "training accuracy 91.7875\n",
      "cost nan\n",
      "training accuracy 91.7875\n",
      "cost nan\n",
      "training accuracy 91.8\n",
      "cost nan\n",
      "training accuracy 91.8\n",
      "cost nan\n",
      "training accuracy 91.79375\n",
      "cost nan\n",
      "training accuracy 91.8\n",
      "cost nan\n",
      "training accuracy 91.8\n",
      "cost nan\n",
      "training accuracy 91.8\n",
      "cost nan\n",
      "training accuracy 91.79375\n",
      "cost nan\n",
      "training accuracy 91.7875\n",
      "cost nan\n",
      "training accuracy 91.79375\n",
      "cost nan\n",
      "training accuracy 91.8\n",
      "cost nan\n",
      "training accuracy 91.8\n",
      "cost nan\n",
      "training accuracy 91.8\n",
      "cost nan\n",
      "training accuracy 91.79375\n",
      "cost nan\n",
      "training accuracy 91.80625\n",
      "cost nan\n",
      "training accuracy 91.81875000000001\n",
      "cost nan\n",
      "training accuracy 91.8375\n",
      "cost nan\n",
      "training accuracy 91.83125\n",
      "cost nan\n",
      "training accuracy 91.8375\n",
      "cost nan\n",
      "training accuracy 91.8375\n",
      "cost nan\n",
      "training accuracy 91.84375\n",
      "cost nan\n",
      "training accuracy 91.84375\n",
      "cost nan\n",
      "training accuracy 91.85\n",
      "cost nan\n",
      "training accuracy 91.85\n"
     ]
    }
   ],
   "source": [
    "for j in range (100):\n",
    "    for i in range(int(len(trainingData)/batch)):\n",
    "        #epoch\n",
    "        #forward pass\n",
    "        #print(w1.shape)\n",
    "        #print(b1.shape)\n",
    "        #print(trainingData[i*batch:(i+1)*batch, :].shape)\n",
    "        a0,a1,a2,z1,z2 = forwardFun(trainingData[i*batch:(i+1)*batch, :])\n",
    "        #backpass\n",
    "        trainingLabelBatch = trainingL[i*batch:(i+1)*batch,:]\n",
    "        y=a2\n",
    "        del2=(y-trainingLabelBatch)    #for last layer\n",
    "        del1=np.dot(del2,w2.T)*(1-pow(actFunc(z1,3),2))\n",
    "        dcdw2=np.dot(a1.T,del2)\n",
    "        dcdw1=np.dot(a0.T,del1)\n",
    "        dcdb1=np.sum(del1,axis=0)\n",
    "        dcdb2=np.sum(del2,axis=0)\n",
    "    #     print(\"del2 = \",del2.shape)\n",
    "    #     print(\"w2 = \",w2.shape)\n",
    "    #     print(\"w1= \",w1.shape)\n",
    "    #     print(\"del1 = \",del1.shape)\n",
    "    #     print(\"a0 = \",a0.shape)\n",
    "    #     print(\"a1 = \",a1.shape)\n",
    "    #     print(\"dcdw1 = \",dcdw1.shape)\n",
    "\n",
    "        w1=w1-alpha*dcdw1\n",
    "        w2=w2-alpha*dcdw2\n",
    "        b1=b1-alpha*dcdb1\n",
    "        b2=b2-alpha*dcdb2\n",
    "\n",
    "    #Cost after one epoch\n",
    "    a0,z1,a1,z2,a2=forwardFun(trainingData)\n",
    "    ytrue=a2\n",
    "    cost=np.sum(CostFun(ytrue,trainingL))/16000\n",
    "    print (\"cost\",cost)\n",
    "    accuracy = accuracyFun(ytrue,trainingL,16000)\n",
    "    print (\"training accuracy\",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
